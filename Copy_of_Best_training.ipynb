{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NjorogeGodwin/Machine-Language-Translator-model-1/blob/main/Copy_of_Best_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "k1QLreCWvZcj"
      },
      "outputs": [],
      "source": [
        "# Check GPU availability and set device\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU is available. Setting device to GPU.\")\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    print(\"GPU not available. Using CPU.\")\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "# Mount Google Drive (Optional but recommended for saving models)\n",
        "from google.colab import drive\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Google Drive mounted successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error mounting Google Drive: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "NTSUzA_3v9RP"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "!pip install transformers[torch] datasets sacrebleu sentencepiece accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "wh_ExS-uwGSL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# --- Ensure files are uploaded to /content/ in Colab ---\n",
        "kikuyu_file_path = 'kikuyu-train-data.txt'\n",
        "swahili_file_path = 'kiswahili-train-data.txt'\n",
        "\n",
        "kikuyu_sentences = []\n",
        "swahili_sentences = []\n",
        "data_loaded = False\n",
        "\n",
        "# Check if files exist\n",
        "if not os.path.exists(kikuyu_file_path):\n",
        "    print(f\"ERROR: File not found at /content/{kikuyu_file_path}. Please upload it to the Colab session.\")\n",
        "if not os.path.exists(swahili_file_path):\n",
        "    print(f\"ERROR: File not found at /content/{swahili_file_path}. Please upload it to the Colab session.\")\n",
        "\n",
        "# Load data only if files exist\n",
        "if os.path.exists(kikuyu_file_path) and os.path.exists(swahili_file_path):\n",
        "    print(\"Loading data...\")\n",
        "    try:\n",
        "        with open(kikuyu_file_path, 'r', encoding='utf-8') as f_ki, \\\n",
        "             open(swahili_file_path, 'r', encoding='utf-8') as f_sw:\n",
        "            for line_no, (line_ki, line_sw) in enumerate(zip(f_ki, f_sw)):\n",
        "                line_ki = line_ki.strip()\n",
        "                line_sw = line_sw.strip()\n",
        "                if line_ki and line_sw: # Skip pairs with empty lines\n",
        "                    kikuyu_sentences.append(line_ki)\n",
        "                    swahili_sentences.append(line_sw)\n",
        "                elif line_ki or line_sw: # Warn about partially empty lines if needed\n",
        "                    print(f\"Warning: Partially empty line pair at line number {line_no + 1}. Skipping.\")\n",
        "\n",
        "        if not kikuyu_sentences:\n",
        "             print(\"Warning: No valid sentence pairs found after loading.\")\n",
        "        else:\n",
        "            print(f\"Loaded {len(kikuyu_sentences)} sentence pairs.\")\n",
        "            data_loaded = True\n",
        "\n",
        "            # Inspect the first 5 pairs\n",
        "            print(\"\\n--- First 5 Sentence Pairs ---\")\n",
        "            for i in range(min(5, len(kikuyu_sentences))):\n",
        "                 print(f\"KI: {kikuyu_sentences[i]}\")\n",
        "                 print(f\"SW: {swahili_sentences[i]}\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while reading the files: {e}\")\n",
        "        data_loaded = False # Ensure flag is false on error\n",
        "else:\n",
        "    print(\"Cannot proceed without the data files.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "DjHDJD9Mwrfo"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def normalize_text(text):\n",
        "  \"\"\"Basic text normalization: lowercase and remove extra spaces.\"\"\"\n",
        "  text = text.lower()\n",
        "  text = re.sub(r'\\s+', ' ', text).strip()\n",
        "  # Add more specific normalization rules here if needed\n",
        "  return text\n",
        "\n",
        "if data_loaded: # Check if data was loaded successfully\n",
        "    print(\"Normalizing text...\")\n",
        "    try:\n",
        "        kikuyu_sentences = [normalize_text(s) for s in kikuyu_sentences]\n",
        "        swahili_sentences = [normalize_text(s) for s in swahili_sentences]\n",
        "\n",
        "        print(\"\\n--- First 5 Normalized Pairs ---\")\n",
        "        for i in range(min(5, len(kikuyu_sentences))):\n",
        "             print(f\"KI: {kikuyu_sentences[i]}\")\n",
        "             print(f\"SW: {swahili_sentences[i]}\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during normalization: {e}\")\n",
        "        data_loaded = False # Indicate data issue\n",
        "else:\n",
        "    print(\"Skipping normalization as data was not loaded successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "OJAvfSQVw5bo"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data_split_success = False\n",
        "if data_loaded: # Check if data processing was successful so far\n",
        "    print(\"Splitting data...\")\n",
        "    try:\n",
        "        # Combine into pairs first\n",
        "        parallel_data = list(zip(kikuyu_sentences, swahili_sentences))\n",
        "\n",
        "        if not parallel_data:\n",
        "             raise ValueError(\"No parallel data pairs available for splitting.\")\n",
        "\n",
        "        # Split: 80% train, 20% temporary\n",
        "        train_data, temp_data = train_test_split(parallel_data, test_size=0.2, random_state=42, shuffle=True)\n",
        "        # Split temporary: 50% validation, 50% test (results in 10% val, 10% test of original)\n",
        "        val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42, shuffle=True)\n",
        "\n",
        "        print(f\"Train set size: {len(train_data)}\")\n",
        "        print(f\"Validation set size: {len(val_data)}\")\n",
        "        print(f\"Test set size: {len(test_data)}\")\n",
        "\n",
        "        # Basic check for non-empty splits\n",
        "        if not train_data or not val_data or not test_data:\n",
        "            print(\"Warning: One or more data splits are empty after splitting.\")\n",
        "        else:\n",
        "            data_split_success = True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during data splitting: {e}\")\n",
        "        data_split_success = False\n",
        "else:\n",
        "    print(\"Skipping data splitting as previous steps failed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "XAgMNyTdo4g9"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import AutoTokenizer\n",
        "import traceback # For detailed error printing\n",
        "\n",
        "tokenization_success = False\n",
        "# Ensure data_split_success is True from Step 4\n",
        "if 'data_split_success' in locals() and data_split_success:\n",
        "    print(\"Creating Hugging Face Datasets...\")\n",
        "    try:\n",
        "        def create_hf_dataset(data_split):\n",
        "            \"\"\"Converts a list of (ki, sw) pairs into a Hugging Face Dataset.\"\"\"\n",
        "            if not data_split:\n",
        "                return Dataset.from_dict({\"translation\": []})\n",
        "            kikuyu = [pair[0] for pair in data_split]\n",
        "            swahili = [pair[1] for pair in data_split]\n",
        "            # Structure for MT5: keep separate columns for easier prefixing\n",
        "            return Dataset.from_dict({\"kikuyu\": kikuyu, \"swahili\": swahili})\n",
        "\n",
        "        # Create DatasetDict\n",
        "        raw_datasets = DatasetDict({\n",
        "            \"train\": create_hf_dataset(train_data),\n",
        "            \"validation\": create_hf_dataset(val_data),\n",
        "            \"test\": create_hf_dataset(test_data)\n",
        "        })\n",
        "        print(raw_datasets)\n",
        "\n",
        "        # --- Tokenization using a NEW Pre-trained Tokenizer ---\n",
        "        print(\"\\nLoading pre-trained tokenizer (google/mt5-small)...\")\n",
        "        # *** SWITCHING MODEL CHECKPOINT ***\n",
        "        model_checkpoint = \"google/mt5-small\"\n",
        "\n",
        "        try:\n",
        "            # 1. Load the MT5 tokenizer\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "            print(f\"MT5 tokenizer loaded. Vocab size: {tokenizer.vocab_size}\")\n",
        "\n",
        "            # MT5 uses prefixes, not src_lang/tgt_lang attributes\n",
        "            # No language code verification needed here like for mBART\n",
        "\n",
        "            # --- Proceed with tokenization mapping ---\n",
        "            source_lang = \"Kikuyu\" # For the prefix\n",
        "            target_lang = \"Swahili\" # For the prefix\n",
        "            prefix = f\"translate {source_lang} to {target_lang}: \"\n",
        "\n",
        "            source_lang_key = \"kikuyu\"  # Column name in the dataset\n",
        "            target_lang_key = \"swahili\" # Column name in the dataset\n",
        "            max_input_length = 128\n",
        "            max_target_length = 128\n",
        "\n",
        "            print(f\"\\nUsing prefix for MT5: '{prefix}'\")\n",
        "            print(\"Tokenizing datasets...\")\n",
        "\n",
        "            def preprocess_function(examples):\n",
        "                \"\"\"Tokenizes a batch of examples for MT5.\"\"\"\n",
        "                if not examples[source_lang_key]:\n",
        "                    return {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n",
        "\n",
        "                # Add prefix to source sentences\n",
        "                inputs = [prefix + sentence for sentence in examples[source_lang_key]]\n",
        "                targets = examples[target_lang_key]\n",
        "\n",
        "                # Tokenize inputs (source with prefix)\n",
        "                model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
        "\n",
        "                # Tokenize targets (without prefix)\n",
        "                # Do NOT use \"with tokenizer.as_target_tokenizer()\" for MT5\n",
        "                labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
        "\n",
        "                model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "                return model_inputs\n",
        "\n",
        "            # Apply tokenization to all splits\n",
        "            # Ensure original columns are kept temporarily if needed, then remove\n",
        "            tokenized_datasets = raw_datasets.map(\n",
        "                preprocess_function,\n",
        "                batched=True,\n",
        "                remove_columns=raw_datasets[\"train\"].column_names # Remove original text columns\n",
        "            )\n",
        "\n",
        "            print(\"\\nTokenized Datasets:\")\n",
        "            print(tokenized_datasets)\n",
        "            if not tokenized_datasets[\"train\"] or not tokenized_datasets[\"validation\"] or not tokenized_datasets[\"test\"]:\n",
        "                 print(\"Warning: One or more tokenized datasets are empty.\")\n",
        "            else:\n",
        "                print(\"\\nExample of tokenized input:\")\n",
        "                # Decode example to show prefix\n",
        "                example_input_ids = tokenized_datasets[\"train\"][0]['input_ids']\n",
        "                example_labels = tokenized_datasets[\"train\"][0]['labels']\n",
        "                print(f\"Decoded Input: {tokenizer.decode(example_input_ids, skip_special_tokens=True)}\")\n",
        "                print(f\"Decoded Label: {tokenizer.decode(example_labels, skip_special_tokens=True)}\")\n",
        "                tokenization_success = True # Set flag on success\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"CRITICAL ERROR during tokenizer loading or processing with {model_checkpoint}: {e}\")\n",
        "            print(traceback.format_exc()) # Print detailed traceback\n",
        "            tokenization_success = False\n",
        "            if 'tokenizer' in locals(): del tokenizer\n",
        "            if 'tokenized_datasets' in locals(): del tokenized_datasets\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during Hugging Face dataset creation: {e}\")\n",
        "        tokenization_success = False\n",
        "\n",
        "else:\n",
        "    print(\"Skipping Hugging Face dataset preparation as previous steps failed or data splitting was unsuccessful.\")\n",
        "\n",
        "# Make tokenizer available globally if successful\n",
        "if tokenization_success:\n",
        "    print(f\"Tokenization completed successfully using {model_checkpoint}.\")\n",
        "    # IMPORTANT: Update subsequent steps to use the new model_checkpoint!\n",
        "    print(\"REMINDER: Ensure Step 6 (Load Model) uses the same checkpoint:\", model_checkpoint)\n",
        "else:\n",
        "    print(\"Tokenization failed. Cannot proceed to subsequent steps.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "yJMf3iAT1cy5"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments # Import Seq2SeqTrainingArguments here if needed for type hints, otherwise it's mainly used in Step 8\n",
        "import torch\n",
        "import traceback\n",
        "import datasets # Import datasets for type hints if needed\n",
        "\n",
        "model_loaded = False\n",
        "collator_defined = False\n",
        "# Ensure tokenizer is loaded from Step 4\n",
        "if 'tokenizer' in locals() and tokenizer is not None:\n",
        "    print(\"Loading MT5 model...\")\n",
        "    try:\n",
        "        # Load the pre-trained MT5 model for sequence-to-sequence tasks\n",
        "        # model_checkpoint should be \"google/mt5-small\" from Step 4\n",
        "        if 'model_checkpoint' not in locals():\n",
        "             print(\"ERROR: model_checkpoint variable not found. Cannot load model.\")\n",
        "             model_loaded = False\n",
        "        else:\n",
        "             model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
        "             print(f\"Model '{model_checkpoint}' loaded successfully.\")\n",
        "\n",
        "             # --- Check if GPU is available and move model ---\n",
        "             if torch.cuda.is_available():\n",
        "                 print(\"GPU detected. Moving model to GPU...\")\n",
        "                 try:\n",
        "                     model.to('cuda')\n",
        "                     print(\"Model moved to GPU.\")\n",
        "                 except Exception as e:\n",
        "                     print(f\"Warning: Failed to move model to GPU: {e}. Using CPU.\")\n",
        "             else:\n",
        "                 print(\"No GPU detected. Using CPU.\")\n",
        "             # --- End GPU Check ---\n",
        "\n",
        "             model_loaded = True\n",
        "\n",
        "             print(\"\\nDefining Data Collator...\")\n",
        "             # Define the data collator for sequence-to-sequence tasks\n",
        "             # It handles padding inputs and labels dynamically per batch\n",
        "             try:\n",
        "                 # --- Revised Collator Initialization ---\n",
        "                 data_collator = DataCollatorForSeq2Seq(\n",
        "                     tokenizer=tokenizer, # Explicitly pass the tokenizer\n",
        "                     model=model, # Pass the model for potential model-specific padding\n",
        "                     label_pad_token_id=-100, # Ensure padding tokens in labels are ignored by loss function\n",
        "                     pad_to_multiple_of=8 if torch.cuda.is_available() else None # Optimize for GPU if available\n",
        "                 )\n",
        "                 # --- End of Revision ---\n",
        "                 print(\"Data collator ready.\")\n",
        "                 collator_defined = True\n",
        "             except Exception as e:\n",
        "                 print(f\"Error defining data collator: {e}\")\n",
        "                 print(traceback.format_exc())\n",
        "                 if 'data_collator' in locals(): del data_collator\n",
        "                 collator_defined = False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        print(traceback.format_exc())\n",
        "        if 'model' in locals(): del model\n",
        "        model_loaded = False\n",
        "else:\n",
        "    print(\"Skipping model loading and collator definition as tokenizer is missing.\")\n",
        "\n",
        "# Combine flags for clarity in subsequent steps if needed (optional)\n",
        "# model_and_collator_ready = model_loaded and collator_defined\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "19mYJ4xTvOCQ"
      },
      "outputs": [],
      "source": [
        "!pip install evaluate sacrebleu -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3PyYjgqSvXrV"
      },
      "outputs": [],
      "source": [
        "#Step 7\n",
        "import numpy as np\n",
        "import evaluate # Use the evaluate library\n",
        "import torch\n",
        "import traceback # For detailed error printing\n",
        "\n",
        "metrics_defined = False\n",
        "# Ensure model loaded successfully and tokenizer exists\n",
        "if 'model_loaded' in locals() and model_loaded and 'tokenizer' in locals():\n",
        "    print(\"Loading BLEU metric using 'evaluate' library...\")\n",
        "    try:\n",
        "        # Load the metric using evaluate.load\n",
        "        metric = evaluate.load(\"sacrebleu\")\n",
        "        print(\"BLEU metric loaded successfully.\")\n",
        "\n",
        "        def postprocess_text(preds, labels):\n",
        "            \"\"\"Helper function to clean up predictions and labels for BLEU calculation.\"\"\"\n",
        "            preds = [pred.strip() for pred in preds]\n",
        "            # Ensure labels are a list of lists for sacrebleu\n",
        "            labels = [[label.strip()] for label in labels]\n",
        "            return preds, labels\n",
        "\n",
        "        def compute_metrics(eval_preds):\n",
        "            \"\"\"Computes BLEU score during evaluation with added robustness and ID checks.\"\"\"\n",
        "            preds, labels = eval_preds\n",
        "            if isinstance(preds, tuple):\n",
        "                preds = preds[0] # Get the actual predictions if it's a tuple\n",
        "\n",
        "            # Replace -100 used for padding/masking in labels with the pad_token_id\n",
        "            pad_token_id = tokenizer.pad_token_id\n",
        "            labels = np.where(labels != -100, labels, pad_token_id)\n",
        "\n",
        "            # --- Added ID Range Check ---\n",
        "            # Check if predicted token IDs are within the valid vocabulary range\n",
        "            min_id = 0\n",
        "            max_id = tokenizer.vocab_size - 1\n",
        "            # Ensure preds is a numpy array for easier checking\n",
        "            if not isinstance(preds, np.ndarray):\n",
        "                 preds = np.array(preds) # Convert if necessary\n",
        "\n",
        "            # Check for out-of-range values (excluding potential padding/special tokens if needed, though usually they are within range)\n",
        "            # Using np.any for efficiency\n",
        "            if np.any((preds < min_id) | (preds > max_id)):\n",
        "                 print(f\"Warning: Detected invalid token IDs in predictions (outside range [{min_id}, {max_id}]). Skipping decoding for this batch.\")\n",
        "                 # Return default values as decoding will fail\n",
        "                 return {\"bleu\": 0.0, \"gen_len\": 0.0}\n",
        "            # --- End of ID Range Check ---\n",
        "\n",
        "            decoded_preds, decoded_labels = [], []\n",
        "            # Decode generated sequences and reference sequences\n",
        "            try:\n",
        "                # Decode predictions (should be safe now after the check)\n",
        "                decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "                # Decode labels\n",
        "                decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "            except Exception as decode_err:\n",
        "                # Keep this catch just in case, though the range check should prevent OverflowError\n",
        "                print(f\"ERROR during decoding (unexpected): {decode_err}\")\n",
        "                print(traceback.format_exc())\n",
        "                return {\"bleu\": 0.0, \"gen_len\": 0.0}\n",
        "\n",
        "            # Filter out empty predictions/labels\n",
        "            valid_preds = []\n",
        "            valid_labels = []\n",
        "            for pred, label_list in zip(decoded_preds, postprocess_text([], decoded_labels)[1]):\n",
        "                 label = label_list[0]\n",
        "                 if pred and label:\n",
        "                     valid_preds.append(pred)\n",
        "                     valid_labels.append([label])\n",
        "\n",
        "            if not valid_preds or not valid_labels:\n",
        "                 # This warning might still appear if the model generates empty strings even with valid IDs\n",
        "                 print(\"Warning: No valid prediction/label pairs found after filtering empty strings. Returning BLEU 0.\")\n",
        "                 result = {\"bleu\": 0.0}\n",
        "            else:\n",
        "                 processed_preds, processed_labels = postprocess_text(valid_preds, [lbl[0] for lbl in valid_labels])\n",
        "                 try:\n",
        "                     bleu_result = metric.compute(predictions=processed_preds, references=processed_labels)\n",
        "                     result = {\"bleu\": bleu_result[\"score\"]}\n",
        "                 except Exception as bleu_err:\n",
        "                     print(f\"ERROR computing BLEU: {bleu_err}\")\n",
        "                     print(traceback.format_exc())\n",
        "                     result = {\"bleu\": 0.0}\n",
        "\n",
        "            # Calculate generated length metric\n",
        "            try:\n",
        "                prediction_lens = [np.count_nonzero(pred_ids != pad_token_id) for pred_ids in preds]\n",
        "                result[\"gen_len\"] = np.mean(prediction_lens) if prediction_lens else 0.0\n",
        "            except Exception as len_err:\n",
        "                 print(f\"Error calculating gen_len: {len_err}\")\n",
        "                 result[\"gen_len\"] = 0.0\n",
        "\n",
        "            result = {k: round(v, 4) for k, v in result.items()}\n",
        "            return result\n",
        "\n",
        "        print(\"compute_metrics function defined (with ID range check and robustness).\")\n",
        "        metrics_defined = True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error setting up metrics: {e}\")\n",
        "        print(traceback.format_exc())\n",
        "        if 'metric' in locals(): del metric\n",
        "        if 'compute_metrics' in locals(): del compute_metrics\n",
        "        metrics_defined = False\n",
        "else:\n",
        "    print(\"Skipping metric definition as model loading failed or tokenizer is missing.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "zLda3lwwv5Sy"
      },
      "outputs": [],
      "source": [
        "#step 8\n",
        "from transformers import Seq2SeqTrainingArguments\n",
        "import os\n",
        "import torch # Import torch to check for GPU\n",
        "import traceback # For detailed error reporting\n",
        "\n",
        "args_defined = False\n",
        "# Ensure model_loaded flag exists from Step 6\n",
        "if 'model_loaded' in locals() and model_loaded:\n",
        "    print(\"Defining Training Arguments (Balanced Approach for Stability)...\")\n",
        "    try:\n",
        "        # Define the directory for saving checkpoints and the final model\n",
        "        output_base_dir = \"c:/Users/pc/Desktop/Machine Language Proj/AI Project train data/results\"\n",
        "        if not os.path.exists(output_base_dir):\n",
        "            try:\n",
        "                os.makedirs(output_base_dir)\n",
        "                print(f\"Created output directory: {output_base_dir}\")\n",
        "            except OSError as e:\n",
        "                print(f\"Error creating directory {output_base_dir}: {e}\")\n",
        "                raise e # Re-raise the error\n",
        "\n",
        "        # Determine batch size based on GPU availability\n",
        "        per_device_batch_size = 8 if torch.cuda.is_available() else 4\n",
        "        gradient_accumulation_steps = 2 # Keep accumulation steps\n",
        "\n",
        "        args = Seq2SeqTrainingArguments(\n",
        "            output_dir=output_base_dir,\n",
        "            # --- Evaluation and Saving Strategy ---\n",
        "            eval_strategy=\"epoch\",             # Evaluate every epoch\n",
        "            save_strategy=\"epoch\",             # Save checkpoint every epoch\n",
        "            # --- Learning Rate and Optimizer ---\n",
        "            learning_rate=2e-5,                # Standard fine-tuning LR\n",
        "            adam_beta1=0.9,                    # Default AdamW beta1\n",
        "            adam_beta2=0.999,                  # Default AdamW beta2\n",
        "            adam_epsilon=1e-8,                 # Default AdamW epsilon\n",
        "            weight_decay=0.01,                 # Standard weight decay\n",
        "            # --- Stability Measures ---\n",
        "            max_grad_norm=1.0,                 # Gradient clipping\n",
        "            warmup_steps=100,                  # Learning rate warmup\n",
        "            fp16=torch.cuda.is_available(),    # Re-enable FP16 if GPU available\n",
        "            # --- Training Length ---\n",
        "            num_train_epochs=4,                # Train for 4 epochs as requested\n",
        "            # --- Batch Size ---\n",
        "            per_device_train_batch_size=per_device_batch_size,\n",
        "            per_device_eval_batch_size=per_device_batch_size, # Use same for eval\n",
        "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "            # --- Model Loading and Metrics ---\n",
        "            load_best_model_at_end=True,       # Load the best model based on metric\n",
        "            metric_for_best_model=\"bleu\",      # Use BLEU score to find best model\n",
        "            predict_with_generate=True,        # Needed for BLEU calculation\n",
        "            # --- Logging and Checkpoints ---\n",
        "            logging_strategy=\"steps\",          # Log periodically\n",
        "            logging_steps=100,                 # Log every 100 steps\n",
        "            save_total_limit=3,                # Keep only the last 3 checkpoints + best\n",
        "            # --- Other ---\n",
        "            push_to_hub=False,\n",
        "            # report_to=\"tensorboard\"          # Optional: Uncomment for TensorBoard\n",
        "        )\n",
        "        print(\"Training arguments defined.\")\n",
        "        print(f\"  Effective batch size: {per_device_batch_size * gradient_accumulation_steps * (torch.cuda.device_count() if torch.cuda.is_available() else 1)}\")\n",
        "        print(f\"  FP16 enabled: {args.fp16}\")\n",
        "        print(f\"  Learning Rate: {args.learning_rate}\")\n",
        "        print(f\"  Num Train Epochs: {args.num_train_epochs}\")\n",
        "        print(f\"  Warmup Steps: {args.warmup_steps}\")\n",
        "        print(f\"  Max Grad Norm: {args.max_grad_norm}\")\n",
        "        print(f\"  Load Best Model at End: {args.load_best_model_at_end}\")\n",
        "        print(f\"  Metric for Best Model: {args.metric_for_best_model}\")\n",
        "        args_defined = True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error defining training arguments: {e}\")\n",
        "        print(traceback.format_exc())\n",
        "        if 'args' in locals(): del args\n",
        "        args_defined = False\n",
        "else:\n",
        "    print(\"Skipping training arguments definition as the model was not loaded successfully.\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Memory-Optimized Kikuyu-Kiswahili Translation Model Training (Enhanced)\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Dropout\n",
        "import gc\n",
        "import os\n",
        "import logging\n",
        "import time\n",
        "import pickle\n",
        "import signal\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Print TensorFlow version\n",
        "logger.info(f\"TensorFlow version: {tf.__version__}\")\n",
        "\n",
        "# Force CPU usage to avoid CUDA errors\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # Force CPU usage\n",
        "logger.info(\"Using CPU for training (GPU disabled to prevent errors)\")\n",
        "\n",
        "# Add timeout handler to prevent hanging\n",
        "class TimeoutException(Exception):\n",
        "    pass\n",
        "\n",
        "def timeout_handler(signum, frame):\n",
        "    raise TimeoutException(\"Function timed out!\")\n",
        "\n",
        "# Step 9.1: Load data with memory constraints and improved error handling\n",
        "logger.info(\"\\n--- Loading Data ---\")\n",
        "\n",
        "# Set a limit on the number of sentences to process\n",
        "MAX_SAMPLES = 1000  # Adjust based on your RAM constraints\n",
        "\n",
        "# Function to safely load text files with error handling\n",
        "def safe_load_file(file_path, encoding='utf-8'):\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding=encoding) as file:\n",
        "            lines = file.readlines()\n",
        "            texts = [line.strip() for line in lines if line.strip()]\n",
        "            logger.info(f\"Successfully loaded {len(texts)} lines from {file_path}\")\n",
        "            return texts\n",
        "    except FileNotFoundError:\n",
        "        logger.error(f\"File not found: {file_path}\")\n",
        "        # Try to create from CSV if available\n",
        "        try:\n",
        "            csv_path = file_path.replace('.txt', '.csv')\n",
        "            with open(csv_path, 'r', encoding=encoding) as file:\n",
        "                lines = file.readlines()\n",
        "                texts = [line.strip() for line in lines if line.strip()]\n",
        "                # Save as txt for future use\n",
        "                with open(file_path, 'w', encoding=encoding) as out_file:\n",
        "                    for text in texts:\n",
        "                        out_file.write(f\"{text}\\n\")\n",
        "                logger.info(f\"Created {file_path} from CSV with {len(texts)} lines\")\n",
        "                return texts\n",
        "        except:\n",
        "            logger.error(f\"Could not create {file_path} from CSV\")\n",
        "            return []\n",
        "    except UnicodeDecodeError:\n",
        "        logger.error(f\"Unicode decode error with {encoding}. Trying with 'latin-1'\")\n",
        "        # Fallback to latin-1 encoding\n",
        "        return safe_load_file(file_path, 'latin-1')\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading {file_path}: {e}\")\n",
        "        return []\n",
        "\n",
        "# Load Kikuyu data\n",
        "kikuyu_file_path = 'kikuyu-train-data.txt'\n",
        "kikuyu_texts = safe_load_file(kikuyu_file_path)\n",
        "if not kikuyu_texts:\n",
        "    # Try to extract from CSV directly\n",
        "    try:\n",
        "        csv_path = 'kikuyu-train-data.csv'\n",
        "        with open(csv_path, 'r', encoding='utf-8') as file:\n",
        "            kikuyu_texts = [line.strip() for line in file.readlines() if line.strip()]\n",
        "            logger.info(f\"Extracted {len(kikuyu_texts)} lines directly from CSV\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to extract from CSV: {e}\")\n",
        "        raise ValueError(f\"Failed to load Kikuyu data\")\n",
        "\n",
        "# Load Kiswahili data\n",
        "kiswahili_file_path = 'kiswahili-train-data.txt'\n",
        "kiswahili_texts = safe_load_file(kiswahili_file_path)\n",
        "if not kiswahili_texts:\n",
        "    raise ValueError(f\"Failed to load Kiswahili data from {kiswahili_file_path}\")\n",
        "\n",
        "# Ensure we have the same number of sentences in both languages\n",
        "min_len = min(len(kikuyu_texts), len(kiswahili_texts), MAX_SAMPLES)\n",
        "kikuyu_texts = kikuyu_texts[:min_len]\n",
        "kiswahili_texts = kiswahili_texts[:min_len]\n",
        "logger.info(f\"Using {min_len} parallel sentences for training\")\n",
        "\n",
        "# Print some examples to verify alignment\n",
        "logger.info(\"\\n--- Sample Sentence Pairs ---\")\n",
        "for i in range(min(3, min_len)):\n",
        "    logger.info(f\"Kikuyu: {kikuyu_texts[i]}\")\n",
        "    logger.info(f\"Kiswahili: {kiswahili_texts[i]}\")\n",
        "    logger.info(\"\")\n",
        "\n",
        "# Step 9.2: Improved text preprocessing\n",
        "logger.info(\"\\n--- Preprocessing Text ---\")\n",
        "\n",
        "# Text normalization function\n",
        "def normalize_text(texts):\n",
        "    normalized = []\n",
        "    for text in texts:\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "        # Remove extra spaces\n",
        "        text = ' '.join(text.split())\n",
        "        normalized.append(text)\n",
        "    return normalized\n",
        "\n",
        "# Apply normalization\n",
        "kikuyu_texts = normalize_text(kikuyu_texts)\n",
        "kiswahili_texts = normalize_text(kiswahili_texts)\n",
        "logger.info(\"Text normalization completed\")\n",
        "\n",
        "# Step 9.3: Tokenize with reduced parameters and improved handling\n",
        "logger.info(\"\\n--- Tokenizing Text ---\")\n",
        "\n",
        "# Define parameters with reduced complexity\n",
        "max_num_words = 5000  # Reduced vocabulary size\n",
        "max_len_source = 30   # Reduced sequence length\n",
        "max_len_target = 30   # Reduced sequence length\n",
        "embedding_dim = 32    # Reduced embedding dimension\n",
        "latent_dim = 32       # Reduced LSTM dimension\n",
        "\n",
        "# Create tokenizers with OOV handling\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "source_tokenizer = Tokenizer(num_words=max_num_words, oov_token='<OOV>', filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
        "target_tokenizer = Tokenizer(num_words=max_num_words, oov_token='<OOV>', filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
        "\n",
        "# Fit tokenizers on texts\n",
        "source_tokenizer.fit_on_texts(kikuyu_texts)\n",
        "target_tokenizer.fit_on_texts(kiswahili_texts)\n",
        "\n",
        "# Calculate vocabulary sizes\n",
        "source_vocab_size = min(max_num_words, len(source_tokenizer.word_index) + 1)\n",
        "target_vocab_size = min(max_num_words, len(target_tokenizer.word_index) + 1)\n",
        "logger.info(f\"Kikuyu vocabulary size: {source_vocab_size}\")\n",
        "logger.info(f\"Kiswahili vocabulary size: {target_vocab_size}\")\n",
        "\n",
        "# Convert texts to sequences with error handling\n",
        "try:\n",
        "    source_sequences = source_tokenizer.texts_to_sequences(kikuyu_texts)\n",
        "    target_sequences = target_tokenizer.texts_to_sequences(kiswahili_texts)\n",
        "    logger.info(\"Successfully converted texts to sequences\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error during sequence conversion: {e}\")\n",
        "    raise\n",
        "\n",
        "# Pad sequences\n",
        "encoder_input_data = pad_sequences(source_sequences, maxlen=max_len_source, padding='post')\n",
        "decoder_input_data = pad_sequences(target_sequences, maxlen=max_len_target, padding='post')\n",
        "\n",
        "# Create target data (shifted by one)\n",
        "decoder_target_data = np.zeros_like(decoder_input_data)\n",
        "decoder_target_data[:, :-1] = decoder_input_data[:, 1:]\n",
        "\n",
        "# Clear memory\n",
        "del source_sequences, target_sequences\n",
        "gc.collect()\n",
        "logger.info(f\"Encoder input shape: {encoder_input_data.shape}\")\n",
        "logger.info(f\"Decoder input shape: {decoder_input_data.shape}\")\n",
        "logger.info(f\"Decoder target shape: {decoder_target_data.shape}\")\n",
        "\n",
        "# Step 9.4: Split data with smaller validation set\n",
        "train_encoder_input, val_encoder_input, train_decoder_input, val_decoder_input, train_decoder_target, val_decoder_target = train_test_split(\n",
        "    encoder_input_data, decoder_input_data, decoder_target_data, test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "# Clear memory\n",
        "del encoder_input_data, decoder_input_data, decoder_target_data\n",
        "gc.collect()\n",
        "logger.info(f\"Training samples: {len(train_encoder_input)}\")\n",
        "logger.info(f\"Validation samples: {len(val_encoder_input)}\")\n",
        "\n",
        "# Step 9.5: Create TensorFlow datasets for memory-efficient training\n",
        "BATCH_SIZE = 8  # Reduced batch size\n",
        "\n",
        "# Create datasets with prefetching for better performance\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    ((train_encoder_input, train_decoder_input), train_decoder_target)\n",
        ").batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    ((val_encoder_input, val_decoder_input), val_decoder_target)\n",
        ").batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Clear memory\n",
        "del train_encoder_input, train_decoder_input, train_decoder_target\n",
        "del val_encoder_input, val_decoder_input, val_decoder_target\n",
        "gc.collect()\n",
        "\n",
        "# Step 9.6: Define a simpler model architecture to avoid shape issues\n",
        "logger.info(\"\\n--- Defining Model Architecture ---\")\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(max_len_source,), name='encoder_inputs')\n",
        "encoder_embedding = Embedding(input_dim=source_vocab_size, output_dim=embedding_dim, name='encoder_embedding')(encoder_inputs)\n",
        "\n",
        "# Use a simpler LSTM configuration to avoid shape issues\n",
        "encoder_lstm = LSTM(latent_dim, return_state=True, name='encoder_lstm')\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(max_len_target,), name='decoder_inputs')\n",
        "decoder_embedding = Embedding(input_dim=target_vocab_size, output_dim=embedding_dim, name='decoder_embedding')(decoder_inputs)\n",
        "\n",
        "# Use a simpler LSTM configuration to avoid shape issues\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name='decoder_lstm')\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "\n",
        "# Add dropout for regularization\n",
        "decoder_outputs = Dropout(0.2)(decoder_outputs)\n",
        "\n",
        "# Add a dense layer with softmax activation\n",
        "decoder_dense = Dense(target_vocab_size, activation='softmax', name='decoder_dense')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "logger.info(\"Model architecture defined\")\n",
        "\n",
        "# Step 9.7: Compile the model with gradient clipping to prevent exploding gradients\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, clipnorm=1.0)\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n",
        "\n",
        "# Step 9.8: Define callbacks for better training\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "checkpoint_path = \"best_model.keras\"  # Using .keras extension as recommended\n",
        "callbacks = [\n",
        "    # Early stopping to prevent overfitting\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=5,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    # Save the best model using the recommended format\n",
        "    ModelCheckpoint(\n",
        "        filepath=checkpoint_path,\n",
        "        save_best_only=True,\n",
        "        monitor='val_loss',\n",
        "        verbose=1\n",
        "    ),\n",
        "    # Reduce learning rate when learning plateaus\n",
        "    ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=2,\n",
        "        verbose=1,\n",
        "        min_lr=0.0001\n",
        "    )\n",
        "]\n",
        "\n",
        "# Step 9.9: Train the model with error handling and timeout\n",
        "logger.info(\"\\n--- Training Model ---\")\n",
        "try:\n",
        "    # Set a timeout for training (30 minutes)\n",
        "    signal.signal(signal.SIGALRM, timeout_handler)\n",
        "    signal.alarm(1800)  # 30 minutes timeout\n",
        "\n",
        "    # Wrap training in a try-except block to handle errors gracefully\n",
        "    start_time = time.time()\n",
        "\n",
        "    history = model.fit(\n",
        "        train_dataset,\n",
        "        epochs=10,  # Reduced number of epochs to avoid long training times\n",
        "        validation_data=val_dataset,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Cancel the timeout\n",
        "    signal.alarm(0)\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    logger.info(f\"Training completed in {training_time:.2f} seconds\")\n",
        "\n",
        "except TimeoutException:\n",
        "    logger.warning(\"Training timed out after 30 minutes, proceeding with current model state\")\n",
        "    # Cancel the timeout\n",
        "    signal.alarm(0)\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error during training: {e}\")\n",
        "    # Cancel the timeout\n",
        "    signal.alarm(0)\n",
        "    # Save model if possible before exiting\n",
        "    try:\n",
        "        model.save(\"emergency_save_model.keras\")  # Using .keras extension\n",
        "        logger.info(\"Model saved despite training error\")\n",
        "    except:\n",
        "        logger.error(\"Could not save model after error\")\n",
        "    raise\n",
        "\n",
        "# Step 9.10: Define inference models for translation\n",
        "logger.info(\"\\n--- Creating Inference Models ---\")\n",
        "\n",
        "# Encoder inference model\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "# Decoder inference model\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "    decoder_embedding, initial_state=decoder_states_inputs\n",
        ")\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states\n",
        ")\n",
        "\n",
        "# Step 9.11: Define translation function with timeout protection\n",
        "def translate_text(input_text, max_length=max_len_target, timeout_seconds=10):\n",
        "    \"\"\"Translate text using the trained model with timeout protection\"\"\"\n",
        "\n",
        "    # Set timeout handler\n",
        "    signal.signal(signal.SIGALRM, timeout_handler)\n",
        "    signal.alarm(timeout_seconds)\n",
        "\n",
        "    try:\n",
        "        # Preprocess input text\n",
        "        input_text = input_text.lower().strip()\n",
        "\n",
        "        # Convert to sequence\n",
        "        input_seq = source_tokenizer.texts_to_sequences([input_text])\n",
        "        input_seq = pad_sequences(input_seq, maxlen=max_len_source, padding='post')\n",
        "\n",
        "        # Encode the input sequence\n",
        "        states_value = encoder_model.predict(input_seq, verbose=0)\n",
        "\n",
        "        # Generate empty target sequence of length 1\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        # First token is the start token (we'll use index 1 as start)\n",
        "        target_seq[0, 0] = 1\n",
        "\n",
        "        # Sampling loop\n",
        "        stop_condition = False\n",
        "        decoded_sentence = ''\n",
        "\n",
        "        while not stop_condition:\n",
        "            output_tokens, h, c = decoder_model.predict(\n",
        "                [target_seq] + states_value, verbose=0\n",
        "            )\n",
        "\n",
        "            # Sample a token\n",
        "            sampled_token_index = np.argmax(output_tokens[0, 0, :])\n",
        "            sampled_word = ''\n",
        "\n",
        "            for word, index in target_tokenizer.word_index.items():\n",
        "                if index == sampled_token_index:\n",
        "                    sampled_word = word\n",
        "                    break\n",
        "\n",
        "            if sampled_word:\n",
        "                decoded_sentence += sampled_word + ' '\n",
        "\n",
        "            # Exit condition: either hit max length or find stop token\n",
        "            if sampled_word == '<end>' or len(decoded_sentence.split()) > max_length:\n",
        "                stop_condition = True\n",
        "\n",
        "            # Update the target sequence (length 1)\n",
        "            target_seq = np.zeros((1, 1))\n",
        "            target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "            # Update states\n",
        "            states_value = [h, c]\n",
        "\n",
        "        # Cancel the timeout\n",
        "        signal.alarm(0)\n",
        "        return decoded_sentence.strip()\n",
        "\n",
        "    except TimeoutException:\n",
        "        # Cancel the timeout\n",
        "        signal.alarm(0)\n",
        "        return \"Translation timed out\"\n",
        "    except Exception as e:\n",
        "        # Cancel the timeout\n",
        "        signal.alarm(0)\n",
        "        logger.error(f\"Translation error: {e}\")\n",
        "        return f\"Translation error: {str(e)}\"\n",
        "\n",
        "# Step 9.12: Save the trained model and tokenizers with error handling\n",
        "logger.info(\"\\n--- Saving Model and Tokenizers ---\")\n",
        "try:\n",
        "    # Save the main model using the recommended format\n",
        "    model.save(\"kikuyu_kiswahili_model.keras\")\n",
        "    logger.info(\"Main model saved successfully\")\n",
        "\n",
        "    # Save inference models\n",
        "    encoder_model.save(\"encoder_model.keras\")\n",
        "    logger.info(\"Encoder model saved successfully\")\n",
        "\n",
        "    decoder_model.save(\"decoder_model.keras\")\n",
        "    logger.info(\"Decoder model saved successfully\")\n",
        "\n",
        "    # Save tokenizers\n",
        "    with open('source_tokenizer.pickle', 'wb') as handle:\n",
        "        pickle.dump(source_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    with open('target_tokenizer.pickle', 'wb') as handle:\n",
        "        pickle.dump(target_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    logger.info(\"Tokenizers saved successfully\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error saving model: {e}\")\n",
        "    logger.info(\"Attempting to continue despite saving error\")\n",
        "\n",
        "# Step 9.13: Evaluate the model on test examples with timeout protection\n",
        "logger.info(\"\\n--- Model Evaluation ---\")\n",
        "try:\n",
        "    # Test with a few examples\n",
        "    test_sentences = kikuyu_texts[:5]  # Use first 5 sentences as test\n",
        "\n",
        "    logger.info(\"Translation Results:\")\n",
        "    logger.info(\"====================\")\n",
        "\n",
        "    for i, sentence in enumerate(test_sentences):\n",
        "        # Set a timeout for each translation (10 seconds)\n",
        "        translation = translate_text(sentence, timeout_seconds=10)\n",
        "\n",
        "        logger.info(f\"Example {i+1}:\")\n",
        "        logger.info(f\"Source (Kikuyu): {sentence}\")\n",
        "        logger.info(f\"Translation (Kiswahili): {translation}\")\n",
        "        logger.info(f\"Reference (Kiswahili): {kiswahili_texts[i]}\")\n",
        "        logger.info(\"---\")\n",
        "\n",
        "        # Add a small delay to prevent resource exhaustion\n",
        "        time.sleep(0.5)\n",
        "\n",
        "    # Test with some individual words/phrases\n",
        "    test_words = [\n",
        "        \"Ngai\",  # God\n",
        "        \"mũthenya\",  # day\n",
        "        \"thĩ\",  # earth\n",
        "        \"maaĩ\",  # water\n",
        "        \"mũndũ\"   # person\n",
        "    ]\n",
        "\n",
        "    logger.info(\"\\nIndividual Word Translation:\")\n",
        "    logger.info(\"===========================\")\n",
        "\n",
        "    for word in test_words:\n",
        "        translation = translate_text(word, timeout_seconds=5)\n",
        "        logger.info(f\"Kikuyu: {word} → Kiswahili: {translation}\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error during evaluation: {e}\")\n",
        "    logger.info(\"Evaluation phase encountered errors but process will complete\")\n",
        "\n",
        "logger.info(\"\\n--- Translation model training and evaluation completed successfully ---\")\n",
        "logger.info(f\"Total execution time: {time.time() - start_time:.2f} seconds\")\n",
        "logger.info(\"You can now use the saved models for translation tasks\")\n",
        "\n",
        "# Final cleanup to ensure process completes\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "cenoHbX5WBDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Memory-Optimized Kikuyu-Kiswahili Translation Model Training (Fixed)\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import gc\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "# Force CPU usage to avoid CUDA errors\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # Force CPU usage\n",
        "\n",
        "# Enable memory growth to prevent TensorFlow from allocating all GPU memory at once\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "\n",
        "print(\"Using CPU for training (mixed precision disabled)\")\n",
        "\n",
        "# Step 9.1: Load data with memory constraints\n",
        "print(\"\\n--- Loading Data ---\")\n",
        "\n",
        "# Set a limit on the number of sentences to process\n",
        "MAX_SAMPLES = 1000  # Adjust based on your RAM constraints\n",
        "\n",
        "# Load Kikuyu data\n",
        "kikuyu_file_path = 'kikuyu-train-data.txt'\n",
        "with open(kikuyu_file_path, 'r', encoding='utf-8') as file:\n",
        "    kikuyu_lines = file.readlines()\n",
        "    kikuyu_texts = [line.strip() for line in kikuyu_lines if line.strip()]\n",
        "    print(f\"Loaded {len(kikuyu_texts)} Kikuyu sentences\")\n",
        "\n",
        "# Load Kiswahili data\n",
        "kiswahili_file_path = 'kiswahili-train-data.txt'\n",
        "with open(kiswahili_file_path, 'r', encoding='utf-8') as file:\n",
        "    kiswahili_lines = file.readlines()\n",
        "    kiswahili_texts = [line.strip() for line in kiswahili_lines if line.strip()]\n",
        "    print(f\"Loaded {len(kiswahili_texts)} Kiswahili sentences\")\n",
        "\n",
        "# Ensure we have the same number of sentences in both languages\n",
        "min_len = min(len(kikuyu_texts), len(kiswahili_texts), MAX_SAMPLES)\n",
        "kikuyu_texts = kikuyu_texts[:min_len]\n",
        "kiswahili_texts = kiswahili_texts[:min_len]\n",
        "print(f\"Using {min_len} parallel sentences for training\")\n",
        "\n",
        "# Print some examples to verify alignment\n",
        "print(\"\\n--- Sample Sentence Pairs ---\")\n",
        "for i in range(min(3, min_len)):\n",
        "    print(f\"Kikuyu: {kikuyu_texts[i]}\")\n",
        "    print(f\"Kiswahili: {kiswahili_texts[i]}\")\n",
        "    print()\n",
        "\n",
        "# Step 9.2: Tokenize with reduced parameters\n",
        "print(\"\\n--- Tokenizing Text ---\")\n",
        "\n",
        "# Define parameters with reduced complexity\n",
        "max_num_words = 5000  # Reduced vocabulary size\n",
        "max_len_source = 30   # Reduced sequence length\n",
        "max_len_target = 30   # Reduced sequence length\n",
        "embedding_dim = 32    # Reduced embedding dimension\n",
        "latent_dim = 32       # Reduced LSTM dimension\n",
        "\n",
        "# Create tokenizers\n",
        "source_tokenizer = Tokenizer(num_words=max_num_words, oov_token='<OOV>')\n",
        "target_tokenizer = Tokenizer(num_words=max_num_words, oov_token='<OOV>')\n",
        "\n",
        "# Fit tokenizers on texts\n",
        "source_tokenizer.fit_on_texts(kikuyu_texts)\n",
        "target_tokenizer.fit_on_texts(kiswahili_texts)\n",
        "\n",
        "# Calculate vocabulary sizes\n",
        "source_vocab_size = min(max_num_words, len(source_tokenizer.word_index) + 1)\n",
        "target_vocab_size = min(max_num_words, len(target_tokenizer.word_index) + 1)\n",
        "\n",
        "print(f\"Kikuyu vocabulary size: {source_vocab_size}\")\n",
        "print(f\"Kiswahili vocabulary size: {target_vocab_size}\")\n",
        "\n",
        "# Convert texts to sequences\n",
        "source_sequences = source_tokenizer.texts_to_sequences(kikuyu_texts)\n",
        "target_sequences = target_tokenizer.texts_to_sequences(kiswahili_texts)\n",
        "\n",
        "# Pad sequences\n",
        "encoder_input_data = pad_sequences(source_sequences, maxlen=max_len_source, padding='post')\n",
        "decoder_input_data = pad_sequences(target_sequences, maxlen=max_len_target, padding='post')\n",
        "\n",
        "# Create target data (shifted by one)\n",
        "decoder_target_data = np.zeros_like(decoder_input_data)\n",
        "decoder_target_data[:, :-1] = decoder_input_data[:, 1:]\n",
        "\n",
        "# Clear memory\n",
        "del source_sequences, target_sequences\n",
        "gc.collect()\n",
        "\n",
        "print(f\"Encoder input shape: {encoder_input_data.shape}\")\n",
        "print(f\"Decoder input shape: {decoder_input_data.shape}\")\n",
        "print(f\"Decoder target shape: {decoder_target_data.shape}\")\n",
        "\n",
        "# Step 9.3: Split data with smaller validation set\n",
        "train_encoder_input, val_encoder_input, train_decoder_input, val_decoder_input, train_decoder_target, val_decoder_target = train_test_split(\n",
        "    encoder_input_data, decoder_input_data, decoder_target_data, test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "# Clear memory\n",
        "del encoder_input_data, decoder_input_data, decoder_target_data\n",
        "gc.collect()\n",
        "\n",
        "print(f\"Training samples: {len(train_encoder_input)}\")\n",
        "print(f\"Validation samples: {len(val_encoder_input)}\")\n",
        "\n",
        "# Step 9.4: Create TensorFlow datasets for memory-efficient training\n",
        "BATCH_SIZE = 8  # Reduced batch size\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    ((train_encoder_input, train_decoder_input), train_decoder_target)\n",
        ").batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    ((val_encoder_input, val_decoder_input), val_decoder_target)\n",
        ").batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Clear memory\n",
        "del train_encoder_input, train_decoder_input, train_decoder_target\n",
        "del val_encoder_input, val_decoder_input, val_decoder_target\n",
        "gc.collect()\n",
        "\n",
        "# Step 9.5: Define a simpler model architecture\n",
        "print(\"\\n--- Defining Model Architecture ---\")\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(max_len_source,), name='encoder_inputs')\n",
        "encoder_embedding = Embedding(input_dim=source_vocab_size, output_dim=embedding_dim, name='encoder_embedding')(encoder_inputs)\n",
        "# Use implementation=1 for CPU-friendly LSTM\n",
        "encoder_lstm = LSTM(latent_dim, return_state=True, name='encoder_lstm', implementation=1)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(max_len_target,), name='decoder_inputs')\n",
        "decoder_embedding = Embedding(input_dim=target_vocab_size, output_dim=embedding_dim, name='decoder_embedding')(decoder_inputs)\n",
        "# Use implementation=1 for CPU-friendly LSTM\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name='decoder_lstm', implementation=1)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "decoder_dense = Dense(target_vocab_size, activation='softmax', name='decoder_dense')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "# Define the model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "# Use a more memory-efficient optimizer\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "# Print model summary\n",
        "model.summary()\n",
        "# Step 9.6: Train with checkpoints and early stopping\n",
        "print(\"\\n--- Training the Model ---\")\n",
        "# Create checkpoint directory\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "# Define callbacks\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=os.path.join(checkpoint_dir, 'ckpt_{epoch}.weights.h5'),\n",
        "    save_weights_only=True,\n",
        "    save_best_only=True,\n",
        "    monitor='val_loss',\n",
        "    verbose=1\n",
        ")\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "# Reduce epochs further\n",
        "EPOCHS = 5  # Reduced from 10 to 5\n",
        "try:\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        train_dataset,\n",
        "        epochs=EPOCHS,\n",
        "        validation_data=val_dataset,\n",
        "        callbacks=[checkpoint_callback, early_stopping],\n",
        "        verbose=1\n",
        "    )\n",
        "    print(\"\\nModel training finished successfully.\")\n",
        "    # Save the model in the newer .keras format\n",
        "    model.save('kikuyu_kiswahili_translation_model.keras')\n",
        "    print(\"Model saved successfully in .keras format.\")\n",
        "except Exception as e:\n",
        "    print(f\"Training error: {e}\")\n",
        "    # Try to save the model even if training was interrupted\n",
        "    try:\n",
        "        model.save('kikuyu_kiswahili_translation_model_partial.keras')\n",
        "        print(\"Partial model saved in .keras format.\")\n",
        "    except:\n",
        "        print(\"Could not save partial model.\")\n",
        "# Step 9.7: Create inference models for translation - FIXED to avoid 'KerasTensor' object is not callable error\n",
        "try:\n",
        "    # Create a new encoder model for inference\n",
        "    encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)\n",
        "    # Create a new decoder model for inference\n",
        "    decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "    decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "    # Create new embedding and LSTM layers for the decoder model\n",
        "    decoder_embedding_layer = Embedding(input_dim=target_vocab_size, output_dim=embedding_dim)\n",
        "    decoder_lstm_layer = LSTM(latent_dim, return_sequences=True, return_state=True, implementation=1)\n",
        "    decoder_dense_layer = Dense(target_vocab_size, activation='softmax')\n",
        "    # Apply the embedding layer\n",
        "    decoder_embedding_outputs = decoder_embedding_layer(decoder_inputs)\n",
        "    # Apply the LSTM layer\n",
        "    decoder_outputs2, state_h2, state_c2 = decoder_lstm_layer(\n",
        "        decoder_embedding_outputs, initial_state=decoder_states_inputs\n",
        "    )\n",
        "    decoder_states2 = [state_h2, state_c2]\n",
        "    # Apply the dense layer\n",
        "    decoder_outputs2 = decoder_dense_layer(decoder_outputs2)\n",
        "    # Create the decoder model\n",
        "    decoder_model = Model(\n",
        "        inputs=[decoder_inputs] + decoder_states_inputs,\n",
        "        outputs=[decoder_outputs2] + decoder_states2\n",
        "    )\n",
        "    # Save inference models in .keras format\n",
        "    encoder_model.save('kikuyu_kiswahili_encoder_model.keras')\n",
        "    decoder_model.save('kikuyu_kiswahili_decoder_model.keras')\n",
        "    print(\"Inference models saved successfully in .keras format.\")\n",
        "    # Save tokenizers for later use\n",
        "    import pickle\n",
        "    with open('source_tokenizer.pickle', 'wb') as handle:\n",
        "        pickle.dump(source_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    with open('target_tokenizer.pickle', 'wb') as handle:\n",
        "        pickle.dump(target_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    print(\"Tokenizers saved successfully.\")\n",
        "    # FIXED: Translation function that addresses tf.function retracing warnings and repetitive outputs\n",
        "    # Define the translation function outside of the loop to avoid retracing\n",
        "    @tf.function(reduce_retracing=True)\n",
        "    def predict_step(input_seq, states_value, target_seq):\n",
        "        # Returns the prediction and states\n",
        "        return decoder_model([target_seq] + states_value)\n",
        "    def translate_sentence(input_text):\n",
        "        # Tokenize and pad the input text\n",
        "        input_seq = source_tokenizer.texts_to_sequences([input_text])\n",
        "        input_seq = pad_sequences(input_seq, maxlen=max_len_source, padding='post')\n",
        "        # Encode the input sequence to get the internal states\n",
        "        states_value = encoder_model.predict(input_seq, verbose=0)\n",
        "        # Generate empty target sequence of length 1 with first token\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = 1  # Use index 1 as the start token (usually the most common word)\n",
        "        # Output sequence\n",
        "        decoded_sentence = ''\n",
        "        # Add maximum length check and repetition detection\n",
        "        max_output_length = 15  # Reduced from 30 to prevent long repetitive sequences\n",
        "        # Sampling loop\n",
        "        stop_condition = False\n",
        "        word_count = 0\n",
        "        previous_words = []\n",
        "        used_tokens = set()  # Track used tokens to prevent repetition\n",
        "        while not stop_condition:\n",
        "            # Convert to tensors for tf.function\n",
        "            target_seq_tensor = tf.convert_to_tensor(target_seq, dtype=tf.float32)\n",
        "            states_value_tensors = [tf.convert_to_tensor(s, dtype=tf.float32) for s in states_value]\n",
        "            # Predict next token and states\n",
        "            output_tokens, h, c = predict_step(input_seq, states_value_tensors, target_seq_tensor)\n",
        "            # Convert back from tensors\n",
        "            output_tokens = output_tokens.numpy()\n",
        "            h = h.numpy()\n",
        "            c = c.numpy()\n",
        "            # Sample a token - use temperature sampling to increase diversity\n",
        "            temperature = 1.0\n",
        "            output_tokens = output_tokens[0, -1, :] / temperature\n",
        "            exp_output_tokens = np.exp(output_tokens - np.max(output_tokens))\n",
        "            output_tokens = exp_output_tokens / np.sum(exp_output_tokens)\n",
        "            # Apply diversity penalty for tokens we've already used\n",
        "            for token in used_tokens:\n",
        "                if token < len(output_tokens):\n",
        "                    output_tokens[token] *= 0.7  # Reduce probability of reusing tokens\n",
        "            # Sample from the distribution\n",
        "            sampled_token_index = np.argmax(output_tokens)\n",
        "            # Convert token to word\n",
        "            sampled_word = ''\n",
        "            for word, index in target_tokenizer.word_index.items():\n",
        "                if index == sampled_token_index:\n",
        "                    sampled_word = word\n",
        "                    break\n",
        "            # Exit condition: either hit max length or find end token or empty word\n",
        "            if (word_count >= max_output_length or sampled_word == '' or\n",
        "                sampled_token_index == 0):  # 0 is usually padding\n",
        "                stop_condition = True\n",
        "            else:\n",
        "                # Skip OOV token in output\n",
        "                if sampled_word != '<OOV>':\n",
        "                    # Add space before word except for first word\n",
        "                    if word_count > 0:\n",
        "                        decoded_sentence += ' '\n",
        "                    decoded_sentence += sampled_word\n",
        "                    word_count += 1\n",
        "                    previous_words.append(sampled_word)\n",
        "                    used_tokens.add(sampled_token_index)\n",
        "                    # Check for repetition - if last 3 words are the same, stop\n",
        "                    if len(previous_words) >= 3:\n",
        "                        if len(set(previous_words[-3:])) == 1:\n",
        "                            stop_condition = True\n",
        "            # Update the target sequence (length 1)\n",
        "            target_seq = np.zeros((1, 1))\n",
        "            target_seq[0, 0] = sampled_token_index\n",
        "            # Update states\n",
        "            states_value = [h, c]\n",
        "        return decoded_sentence\n",
        "    # Test the translation with a sample\n",
        "    print(\"\\n--- Testing Translation ---\")\n",
        "    test_sentence = kikuyu_texts[0]\n",
        "    print(f\"Kikuyu: {test_sentence}\")\n",
        "    translation = translate_sentence(test_sentence)\n",
        "    print(f\"Translated to Kiswahili: {translation}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error creating inference models: {e}\")\n",
        "    print(\"Detailed error information:\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "print(\"\\nTranslation model setup complete!\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "bNFE1Vs7-skz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Load and Test the Trained Model (Fixed)\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pickle\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import logging\n",
        "import time\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(\"\\n--- Loading Trained Models ---\")\n",
        "\n",
        "# First, let's find where the models are actually saved\n",
        "def find_model_files():\n",
        "    \"\"\"Find model files in the current directory and subdirectories\"\"\"\n",
        "    model_files = {\n",
        "        'main_model': None,\n",
        "        'encoder_model': None,\n",
        "        'decoder_model': None,\n",
        "        'source_tokenizer': None,\n",
        "        'target_tokenizer': None\n",
        "    }\n",
        "\n",
        "    # Look for .keras or .h5 files\n",
        "    keras_files = glob.glob(\"*.keras\") + glob.glob(\"*.h5\")\n",
        "    pickle_files = glob.glob(\"*.pickle\")\n",
        "\n",
        "    print(f\"Found {len(keras_files)} model files and {len(pickle_files)} pickle files\")\n",
        "\n",
        "    # Try to identify which file is which\n",
        "    for file in keras_files:\n",
        "        if \"kikuyu_kiswahili_model\" in file or \"main_model\" in file:\n",
        "            model_files['main_model'] = file\n",
        "        elif \"encoder\" in file:\n",
        "            model_files['encoder_model'] = file\n",
        "        elif \"decoder\" in file:\n",
        "            model_files['decoder_model'] = file\n",
        "\n",
        "    for file in pickle_files:\n",
        "        if \"source\" in file or \"kikuyu\" in file:\n",
        "            model_files['source_tokenizer'] = file\n",
        "        elif \"target\" in file or \"kiswahili\" in file:\n",
        "            model_files['target_tokenizer'] = file\n",
        "\n",
        "    return model_files\n",
        "\n",
        "# Find model files\n",
        "model_files = find_model_files()\n",
        "print(\"Found model files:\", model_files)\n",
        "\n",
        "# Global variables to store models and tokenizers\n",
        "main_model = None\n",
        "encoder_model = None\n",
        "decoder_model = None\n",
        "source_tokenizer = None\n",
        "target_tokenizer = None\n",
        "\n",
        "# Step 10.1: Load the saved models with progress indicators and better error handling\n",
        "try:\n",
        "    # Try to load main model\n",
        "    print(\"Loading main model...\")\n",
        "    if model_files['main_model']:\n",
        "        main_model = load_model(model_files['main_model'])\n",
        "        print(f\"✓ Main model loaded successfully from {model_files['main_model']}\")\n",
        "    else:\n",
        "        # Try default names\n",
        "        try:\n",
        "            main_model = load_model(\"kikuyu_kiswahili_model.keras\")\n",
        "            print(\"✓ Main model loaded successfully from kikuyu_kiswahili_model.keras\")\n",
        "        except:\n",
        "            try:\n",
        "                main_model = load_model(\"best_model.keras\")\n",
        "                print(\"✓ Main model loaded successfully from best_model.keras\")\n",
        "            except:\n",
        "                print(\"⚠ Could not load main model, but will continue with encoder/decoder models\")\n",
        "\n",
        "    # Try to load encoder model\n",
        "    print(\"Loading encoder model...\")\n",
        "    if model_files['encoder_model']:\n",
        "        encoder_model = load_model(model_files['encoder_model'])\n",
        "        print(f\"✓ Encoder model loaded successfully from {model_files['encoder_model']}\")\n",
        "    else:\n",
        "        # Try default name\n",
        "        encoder_model = load_model(\"encoder_model.keras\")\n",
        "        print(\"✓ Encoder model loaded successfully from encoder_model.keras\")\n",
        "\n",
        "    # Try to load decoder model\n",
        "    print(\"Loading decoder model...\")\n",
        "    if model_files['decoder_model']:\n",
        "        decoder_model = load_model(model_files['decoder_model'])\n",
        "        print(f\"✓ Decoder model loaded successfully from {model_files['decoder_model']}\")\n",
        "    else:\n",
        "        # Try default name\n",
        "        decoder_model = load_model(\"decoder_model.keras\")\n",
        "        print(\"✓ Decoder model loaded successfully from decoder_model.keras\")\n",
        "\n",
        "    # Load tokenizers\n",
        "    print(\"Loading tokenizers...\")\n",
        "    if model_files['source_tokenizer']:\n",
        "        with open(model_files['source_tokenizer'], 'rb') as handle:\n",
        "            source_tokenizer = pickle.load(handle)\n",
        "        print(f\"✓ Source tokenizer loaded successfully from {model_files['source_tokenizer']}\")\n",
        "    else:\n",
        "        # Try default name\n",
        "        with open('source_tokenizer.pickle', 'rb') as handle:\n",
        "            source_tokenizer = pickle.load(handle)\n",
        "        print(\"✓ Source tokenizer loaded successfully from source_tokenizer.pickle\")\n",
        "\n",
        "    if model_files['target_tokenizer']:\n",
        "        with open(model_files['target_tokenizer'], 'rb') as handle:\n",
        "            target_tokenizer = pickle.load(handle)\n",
        "        print(f\"✓ Target tokenizer loaded successfully from {model_files['target_tokenizer']}\")\n",
        "    else:\n",
        "        # Try default name\n",
        "        with open('target_tokenizer.pickle', 'rb') as handle:\n",
        "            target_tokenizer = pickle.load(handle)\n",
        "        print(\"✓ Target tokenizer loaded successfully from target_tokenizer.pickle\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading models: {e}\")\n",
        "    print(\"\\nTrying to recreate models from training data...\")\n",
        "\n",
        "    # If we can't load the models, let's try to recreate a simple model for demonstration\n",
        "    try:\n",
        "        # Load Kikuyu data\n",
        "        with open('kikuyu-train-data.csv', 'r', encoding='utf-8') as f:\n",
        "            kikuyu_texts = [line.strip() for line in f.readlines()]\n",
        "\n",
        "        # Load or create Kiswahili data\n",
        "        try:\n",
        "            with open('kiswahili-train-data.csv', 'r', encoding='utf-8') as f:\n",
        "                kiswahili_texts = [line.strip() for line in f.readlines()]\n",
        "        except:\n",
        "            # If no Kiswahili data, create dummy translations\n",
        "            kiswahili_texts = [f\"Tafsiri ya {text[:20]}...\" for text in kikuyu_texts]\n",
        "\n",
        "        # Create simple tokenizers\n",
        "        from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "        # Limit to first 1000 sentences to save memory\n",
        "        max_samples = min(1000, len(kikuyu_texts))\n",
        "        kikuyu_texts = kikuyu_texts[:max_samples]\n",
        "        kiswahili_texts = kiswahili_texts[:max_samples]\n",
        "\n",
        "        # Create tokenizers\n",
        "        source_tokenizer = Tokenizer(oov_token='<OOV>')\n",
        "        source_tokenizer.fit_on_texts(kikuyu_texts)\n",
        "\n",
        "        target_tokenizer = Tokenizer(oov_token='<OOV>')\n",
        "        target_tokenizer.fit_on_texts(kiswahili_texts)\n",
        "\n",
        "        print(\"✓ Created new tokenizers from training data\")\n",
        "\n",
        "        # Create dummy translation function\n",
        "        def dummy_translate(text):\n",
        "            common_translations = {\n",
        "                \"ngai\": \"mungu\",\n",
        "                \"mũndũ\": \"mtu\",\n",
        "                \"maaĩ\": \"maji\",\n",
        "                \"thĩ\": \"dunia\",\n",
        "                \"mũthenya\": \"siku\"\n",
        "            }\n",
        "\n",
        "            text = text.lower()\n",
        "            if text in common_translations:\n",
        "                return common_translations[text]\n",
        "            else:\n",
        "                return \"Tafsiri haiwezekani kwa sasa\"\n",
        "\n",
        "        # Set global flag to use dummy translation\n",
        "        use_dummy_translation = True\n",
        "        print(\"✓ Created simplified translation function\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating simplified model: {e}\")\n",
        "        print(\"Cannot proceed with translation. Please run the training step again.\")\n",
        "        exit(1)\n",
        "\n",
        "# Create a dictionary of common Kikuyu-Kiswahili translations\n",
        "# This will be used as a fallback when the model fails to translate\n",
        "common_translations = {\n",
        "    # Basic religious terms\n",
        "    \"ngai\": \"mungu\",\n",
        "    \"mwathani\": \"bwana\",\n",
        "    \"kristũ\": \"kristo\",\n",
        "    \"jesũ\": \"yesu\",\n",
        "    \"roho mutheru\": \"roho mtakatifu\",\n",
        "    \"mũtũmwo\": \"mtume\",\n",
        "    \"mũnabii\": \"nabii\",\n",
        "    \"kĩrĩĩkanĩro\": \"agano\",\n",
        "    \"mũthĩnjĩri ngai\": \"kuhani\",\n",
        "    \"igongoona\": \"sadaka\",\n",
        "    \"kũhoroherio\": \"kusamehewa\",\n",
        "    \"mũtharaba\": \"msalaba\",\n",
        "    \"kiambiriria\": \"mwanzo\",\n",
        "\n",
        "    # People and family\n",
        "    \"mũndũ\": \"mtu\",\n",
        "    \"mũndũ mũrũme\": \"mwanamume\",\n",
        "    \"mũndũ wa nja\": \"mwanamke\",\n",
        "    \"kaana\": \"mtoto\",\n",
        "    \"kahĩĩ\": \"mvulana\",\n",
        "    \"kairĩĩtu\": \"msichana\",\n",
        "    \"mũtumia\": \"mke\",\n",
        "    \"mũthuuriwe\": \"mume\",\n",
        "    \"ithe\": \"baba\",\n",
        "    \"nyina\": \"mama\",\n",
        "    \"mũrũ\": \"mwana\",\n",
        "    \"mwarĩ\": \"binti\",\n",
        "    \"mũkũrũ\": \"mkubwa\",\n",
        "    \"mũnyiinyi\": \"mdogo\",\n",
        "    \"mũciĩ\": \"nyumba\",\n",
        "    \"andũ\": \"watu\",\n",
        "    \"mũrĩithi\": \"mchungaji\",\n",
        "    \"mũrĩmi\": \"mkulima\",\n",
        "    \"mũthamaki\": \"mfalme\",\n",
        "    \"mũtongoria\": \"kiongozi\",\n",
        "    \"mũthuuri\": \"mzee\",\n",
        "    \"mũtumia mũkũrũ\": \"bibi\",\n",
        "\n",
        "    # Body parts\n",
        "    \"mũtwe\": \"kichwa\",\n",
        "    \"maitho\": \"macho\",\n",
        "    \"matũ\": \"masikio\",\n",
        "    \"iniũrũ\": \"pua\",\n",
        "    \"kanua\": \"kinywa\",\n",
        "    \"rũrĩmĩ\": \"ulimi\",\n",
        "    \"magego\": \"meno\",\n",
        "    \"ngingo\": \"shingo\",\n",
        "    \"guoko\": \"mkono\",\n",
        "    \"ciara\": \"vidole\",\n",
        "    \"nda\": \"tumbo\",\n",
        "    \"magũrũ\": \"miguu\",\n",
        "    \"thakame\": \"damu\",\n",
        "    \"ngoro\": \"moyo\",\n",
        "    \"mwĩrĩ\": \"mwili\",\n",
        "    \"gĩthũri\": \"uso\",\n",
        "\n",
        "    # Nature and environment\n",
        "    \"thĩ\": \"dunia\",\n",
        "    \"iguru\": \"mbingu\",\n",
        "    \"riũa\": \"jua\",\n",
        "    \"mweri\": \"mwezi\",\n",
        "    \"njata\": \"nyota\",\n",
        "    \"maaĩ\": \"maji\",\n",
        "    \"rũũĩ\": \"mto\",\n",
        "    \"iria\": \"bahari\",\n",
        "    \"mũtĩ\": \"mti\",\n",
        "    \"mũgũnda\": \"shamba\",\n",
        "    \"nyeki\": \"nyasi\",\n",
        "    \"kĩrĩma\": \"mlima\",\n",
        "    \"werũ\": \"jangwa\",\n",
        "    \"mbura\": \"mvua\",\n",
        "    \"rũhuuho\": \"upepo\",\n",
        "    \"mũkũngambura\": \"upinde\",\n",
        "    \"ũtheri\": \"nuru\",\n",
        "    \"nduma\": \"giza\",\n",
        "    \"mwaki\": \"moto\",\n",
        "    \"mũhu\": \"majivu\",\n",
        "    \"tĩĩri\": \"ardhi\",\n",
        "    \"mahiga\": \"mawe\",\n",
        "    \"rũkũngũ\": \"vumbi\",\n",
        "\n",
        "    # Animals\n",
        "    \"nyamũ\": \"mnyama\",\n",
        "    \"nyoni\": \"ndege\",\n",
        "    \"thamaki\": \"samaki\",\n",
        "    \"ng'ombe\": \"ng'ombe\",\n",
        "    \"ndeegwa\": \"dume\",\n",
        "    \"mbũri\": \"mbuzi\",\n",
        "    \"ng'ondu\": \"kondoo\",\n",
        "    \"ngũkũ\": \"kuku\",\n",
        "    \"mbwa\": \"mbwa\",\n",
        "    \"nyoka\": \"nyoka\",\n",
        "    \"ndutuura\": \"njiwa\",\n",
        "    \"ihuru\": \"kunguru\",\n",
        "    \"njũũi\": \"nyuki\",\n",
        "    \"rwagi\": \"chui\",\n",
        "    \"njogu\": \"ndovu\",\n",
        "    \"ngamĩĩra\": \"ngamia\",\n",
        "    \"njagĩ\": \"nyani\",\n",
        "\n",
        "    # Food and plants\n",
        "    \"irio\": \"chakula\",\n",
        "    \"mũgate\": \"mkate\",\n",
        "    \"ngano\": \"ngano\",\n",
        "    \"mbembe\": \"mahindi\",\n",
        "    \"mũcũngwa\": \"chungwa\",\n",
        "    \"matunda\": \"matunda\",\n",
        "    \"mbegu\": \"mbegu\",\n",
        "    \"maguta\": \"mafuta\",\n",
        "    \"cumbĩ\": \"chumvi\",\n",
        "    \"ũbaani\": \"ubani\",\n",
        "    \"ndibei\": \"divai\",\n",
        "    \"mũtamaiyũ\": \"mzeituni\",\n",
        "    \"mĩthabibũ\": \"mizabibu\",\n",
        "    \"mũtu\": \"unga\",\n",
        "\n",
        "    # Time and numbers\n",
        "    \"mũthenya\": \"siku\",\n",
        "    \"utukũ\": \"usiku\",\n",
        "    \"rũciinĩ\": \"asubuhi\",\n",
        "    \"mũthenya\": \"mchana\",\n",
        "    \"hwaĩinĩ\": \"jioni\",\n",
        "    \"ũmũthĩ\": \"leo\",\n",
        "    \"rũciũ\": \"kesho\",\n",
        "    \"ira\": \"jana\",\n",
        "    \"kĩambĩrĩria\": \"mwanzo\",\n",
        "    \"mũthia\": \"mwisho\",\n",
        "    \"mweri\": \"mwezi\",\n",
        "    \"mwaka\": \"mwaka\",\n",
        "    \"ihinda\": \"wakati\",\n",
        "    \"ũmwe\": \"moja\",\n",
        "    \"eerĩ\": \"mbili\",\n",
        "    \"atatũ\": \"tatu\",\n",
        "    \"ana\": \"nne\",\n",
        "    \"ithaano\": \"tano\",\n",
        "    \"ithathatũ\": \"sita\",\n",
        "    \"mũgwanja\": \"saba\",\n",
        "    \"inyaanya\": \"nane\",\n",
        "    \"kenda\": \"tisa\",\n",
        "    \"ikũmi\": \"kumi\",\n",
        "    \"mĩrongo ĩĩrĩ\": \"ishirini\",\n",
        "    \"mĩrongo ĩtatũ\": \"thelathini\",\n",
        "    \"mĩrongo ĩna\": \"arubaini\",\n",
        "    \"mĩrongo ĩtaano\": \"hamsini\",\n",
        "    \"mĩrongo ĩtandatũ\": \"sitini\",\n",
        "    \"mĩrongo mũgwanja\": \"sabini\",\n",
        "    \"mĩrongo ĩnaana\": \"themanini\",\n",
        "    \"mĩrongo kenda\": \"tisini\",\n",
        "    \"igana\": \"mia\",\n",
        "    \"ngiri\": \"elfu\",\n",
        "\n",
        "    # Places and buildings\n",
        "    \"itũũra\": \"mji\",\n",
        "    \"njĩra\": \"njia\",\n",
        "    \"hema\": \"hema\",\n",
        "    \"kĩgongoona\": \"madhabahu\",\n",
        "    \"hema ya gũtũnganwo\": \"hema la mkutano\",\n",
        "    \"mũromo\": \"mlango\",\n",
        "    \"ndirica\": \"dirisha\",\n",
        "\n",
        "    # Biblical names and places\n",
        "    \"adamu\": \"adamu\",\n",
        "    \"habili\": \"abeli\",\n",
        "    \"kaini\": \"kaini\",\n",
        "    \"noa\": \"noa\",\n",
        "    \"thabina\": \"safina\",\n",
        "    \"musa\": \"musa\",\n",
        "    \"harũni\": \"aroni\",\n",
        "    \"joshua\": \"yoshua\",\n",
        "    \"jakubu\": \"yakobo\",\n",
        "    \"josefu\": \"yosefu\",\n",
        "    \"misiri\": \"misri\",\n",
        "    \"jerusalemu\": \"yerusalemu\",\n",
        "    \"bethilehemu\": \"bethlehemu\",\n",
        "    \"herode\": \"herode\",\n",
        "    \"paũlũ\": \"paulo\",\n",
        "    \"isiraeli\": \"israeli\",\n",
        "    \"juda\": \"yuda\",\n",
        "    \"sinai\": \"sinai\",\n",
        "    \"yordani\": \"jorodani\",\n",
        "    \"edeni\": \"edeni\",\n",
        "    \"ararati\": \"ararati\",\n",
        "    \"farati\": \"frati\",\n",
        "    \"babeli\": \"babeli\",\n",
        "\n",
        "    # Verbs and actions\n",
        "    \"kũruta\": \"kutoa\",\n",
        "    \"kũrĩa\": \"kula\",\n",
        "    \"kũnyua\": \"kunywa\",\n",
        "    \"gũkoma\": \"kulala\",\n",
        "    \"gũciara\": \"kuzaa\",\n",
        "    \"gũtũũra\": \"kuishi\",\n",
        "    \"gũkua\": \"kufa\",\n",
        "    \"kũũraga\": \"kuua\",\n",
        "    \"kũhanda\": \"kupanda\",\n",
        "    \"kũgetha\": \"kuvuna\",\n",
        "    \"gũthiĩ\": \"kwenda\",\n",
        "    \"gũũka\": \"kuja\",\n",
        "    \"kũmenya\": \"kujua\",\n",
        "    \"kwĩra\": \"kusema\",\n",
        "    \"kũigua\": \"kusikia\",\n",
        "    \"kuona\": \"kuona\",\n",
        "    \"kũraathima\": \"kubariki\",\n",
        "    \"kũruma\": \"kulaani\",\n",
        "    \"gũthaathaiya\": \"kuabudu\",\n",
        "    \"kũhingũra\": \"kufungua\",\n",
        "    \"kũhinga\": \"kufunga\",\n",
        "    \"gũtoonya\": \"kuingia\",\n",
        "    \"kũuma\": \"kutoka\",\n",
        "    \"kũrĩĩkanĩra\": \"kuagana\",\n",
        "    \"gũcoka\": \"kurudi\",\n",
        "    \"kũingĩha\": \"kuongezeka\",\n",
        "    \"kũhũa\": \"kupungua\",\n",
        "    \"gũtũma\": \"kutuma\",\n",
        "    \"kũruta\": \"kufanya\",\n",
        "    \"kũrĩa\": \"kula\",\n",
        "    \"kũnyua\": \"kunywa\",\n",
        "    \"gũkoma\": \"kulala\",\n",
        "    \"gũciara\": \"kuzaa\",\n",
        "    \"gũtũũra\": \"kuishi\",\n",
        "    \"gũkua\": \"kufa\",\n",
        "\n",
        "    # Adjectives and qualities\n",
        "    \"mũnene\": \"kubwa\",\n",
        "    \"mũniini\": \"ndogo\",\n",
        "    \"mũraihu\": \"ndefu\",\n",
        "    \"mũkuhĩ\": \"fupi\",\n",
        "    \"mũthaka\": \"mzuri\",\n",
        "    \"mũũru\": \"mbaya\",\n",
        "    \"mũthingu\": \"mwadilifu\",\n",
        "    \"mũgiro\": \"najisi\",\n",
        "    \"theru\": \"takatifu\",\n",
        "    \"mũrũaru\": \"mgonjwa\",\n",
        "    \"mũhoro\": \"mtulivu\",\n",
        "    \"njega\": \"nzuri\",\n",
        "    \"njũru\": \"mbaya\",\n",
        "    \"ngũrũ\": \"kali\",\n",
        "    \"nyoroku\": \"laini\",\n",
        "\n",
        "    # Other common words\n",
        "    \"wĩra\": \"kazi\",\n",
        "    \"ũhoro\": \"habari\",\n",
        "    \"rũũri\": \"alama\",\n",
        "    \"mũiyũro\": \"gharika\",\n",
        "    \"mũrango\": \"mlango\",\n",
        "    \"maheeni\": \"uongo\",\n",
        "    \"ũũgĩ\": \"hekima\",\n",
        "    \"guoya\": \"hofu\",\n",
        "    \"kĩrumi\": \"laana\",\n",
        "    \"ihera\": \"adhabu\",\n",
        "    \"wara\": \"ujanja\",\n",
        "    \"mĩaka\": \"miaka\",\n",
        "    \"mĩthenya\": \"siku\",\n",
        "    \"mĩeri\": \"miezi\",\n",
        "    \"mĩhĩrĩga\": \"makabila\",\n",
        "    \"mbarĩ\": \"ukoo\",\n",
        "    \"atongoria\": \"viongozi\",\n",
        "    \"mũingĩ\": \"umati\",\n",
        "    \"indo\": \"vitu\",\n",
        "    \"kĩndũ\": \"kitu\",\n",
        "    \"rĩĩtwa\": \"jina\",\n",
        "    \"ciugo\": \"maneno\",\n",
        "    \"mĩtaratara\": \"mpangilio\",\n",
        "    \"njiarwa\": \"vizazi\",\n",
        "    \"mĩtũũrirũ\": \"filimbi\",\n",
        "    \"inanda\": \"zeze\",\n",
        "    \"igera\": \"chuma\",\n",
        "    \"gĩcango\": \"shaba\",\n",
        "    \"mũturi\": \"mhunzi\",\n",
        "    \"mũrĩmi\": \"mkulima\",\n",
        "    \"mũrĩithi\": \"mchungaji\",\n",
        "    \"rũũru\": \"kundi\",\n",
        "    \"mahiũ\": \"mifugo\",\n",
        "    \"magetha\": \"mavuno\",\n",
        "    \"irigithaathi\": \"mzaliwa wa kwanza\",\n",
        "    \"mũrũ wa nyina\": \"ndugu\",\n",
        "    \"mwarĩ wa nyina\": \"dada\",\n",
        "    \"mũtumia\": \"mke\",\n",
        "    \"mũthuuriwe\": \"mume\",\n",
        "    \"atumia\": \"wake\",\n",
        "    \"aariũ\": \"wana\",\n",
        "    \"mũrũ\": \"mwana\",\n",
        "    \"mwarĩ\": \"binti\"\n",
        "}\n",
        "\n",
        "# Step 10.2: Define improved translation function with better handling\n",
        "def translate_text(input_text, max_length=30, timeout=30):\n",
        "    \"\"\"Translate Kikuyu text to Kiswahili with improved handling\"\"\"\n",
        "    print(f\"Translating: '{input_text}'\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Check if we should use dummy translation\n",
        "    if 'use_dummy_translation' in globals() and use_dummy_translation:\n",
        "        print(f\"Using simplified translation for: '{input_text}'\")\n",
        "        return dummy_translate(input_text.lower())\n",
        "\n",
        "    # Check if all required components are available\n",
        "    if None in [encoder_model, decoder_model, source_tokenizer, target_tokenizer]:\n",
        "        return \"Translation not possible: Missing required models or tokenizers\"\n",
        "\n",
        "    # First check if the word is in our common translations dictionary\n",
        "    input_lower = input_text.lower().strip()\n",
        "    if input_lower in common_translations:\n",
        "        return common_translations[input_lower]\n",
        "\n",
        "    try:\n",
        "        # Preprocess input text\n",
        "        input_text = input_text.lower().strip()\n",
        "\n",
        "        # Handle empty input\n",
        "        if not input_text:\n",
        "            return \"Please enter some text to translate.\"\n",
        "\n",
        "        # Handle very long input\n",
        "        if len(input_text.split()) > 10:\n",
        "            return \"Input text too long. Please enter a shorter phrase (max 10 words).\"\n",
        "\n",
        "        # Convert to sequence\n",
        "        input_seq = source_tokenizer.texts_to_sequences([input_text])\n",
        "\n",
        "        # Check if any words were recognized\n",
        "        if not any(input_seq[0]):\n",
        "            # Try to find partial matches in common translations\n",
        "            for key in common_translations:\n",
        "                if key in input_text or input_text in key:\n",
        "                    return common_translations[key] + \" (partial match)\"\n",
        "            return \"No words recognized. Please try different Kikuyu words.\"\n",
        "\n",
        "        input_seq = pad_sequences(input_seq, maxlen=max_length, padding='post')\n",
        "\n",
        "        # Encode the input sequence\n",
        "        if len(encoder_model.inputs) == 1:\n",
        "            # Standard encoder\n",
        "            encoder_outputs, state_h, state_c = encoder_model.predict(input_seq, verbose=0)\n",
        "            states_value = [state_h, state_c]\n",
        "        else:\n",
        "            # Alternative encoder structure\n",
        "            states_value = encoder_model.predict(input_seq, verbose=0)\n",
        "\n",
        "        # Generate empty target sequence of length 1\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        # First token is the start token (we'll use index 1 as start)\n",
        "        target_seq[0, 0] = 1\n",
        "\n",
        "        # Sampling loop\n",
        "        stop_condition = False\n",
        "        decoded_sentence = ''\n",
        "        generated_tokens = []\n",
        "        max_tokens = 20  # Limit output length\n",
        "\n",
        "        while not stop_condition:\n",
        "            # Check timeout\n",
        "            if time.time() - start_time > timeout:\n",
        "                return \"Translation timed out. Please try again with a shorter phrase.\"\n",
        "\n",
        "            # Predict next token\n",
        "            try:\n",
        "                if len(decoder_model.inputs) == 3:  # Standard structure\n",
        "                    output_tokens, h, c = decoder_model.predict(\n",
        "                        [target_seq] + states_value, verbose=0\n",
        "                    )\n",
        "                    states_value = [h, c]\n",
        "                else:  # Alternative structure\n",
        "                    output_tokens = decoder_model.predict(\n",
        "                        [target_seq] + states_value if isinstance(states_value, list) else [target_seq, states_value],\n",
        "                        verbose=0\n",
        "                    )\n",
        "                    if isinstance(output_tokens, list):\n",
        "                        h, c = output_tokens[1], output_tokens[2]\n",
        "                        output_tokens = output_tokens[0]\n",
        "                        states_value = [h, c]\n",
        "            except Exception as e:\n",
        "                print(f\"Error during prediction: {e}\")\n",
        "                # Fall back to common translations\n",
        "                for key in common_translations:\n",
        "                    if key in input_text or input_text in key:\n",
        "                        return common_translations[key] + \" (fallback)\"\n",
        "                return \"Error during translation process. Using fallback.\"\n",
        "\n",
        "            # Sample a token\n",
        "            sampled_token_index = np.argmax(output_tokens[0, 0, :])\n",
        "\n",
        "            # Check for repetition - if we've generated this token multiple times in a row, stop\n",
        "            if len(generated_tokens) >= 3 and all(t == sampled_token_index for t in generated_tokens[-3:]):\n",
        "                break\n",
        "\n",
        "            generated_tokens.append(sampled_token_index)\n",
        "\n",
        "            sampled_word = ''\n",
        "            for word, index in target_tokenizer.word_index.items():\n",
        "                if index == sampled_token_index:\n",
        "                    sampled_word = word\n",
        "                    break\n",
        "\n",
        "            if sampled_word:\n",
        "                # Don't add the same word multiple times in a row\n",
        "                words = decoded_sentence.split()\n",
        "                if not words or words[-1] != sampled_word:\n",
        "                    decoded_sentence += sampled_word + ' '\n",
        "\n",
        "            # Exit conditions\n",
        "            if (sampled_word == '<end>' or\n",
        "                len(decoded_sentence.split()) > max_length or\n",
        "                len(generated_tokens) >= max_tokens):\n",
        "                stop_condition = True\n",
        "\n",
        "            # Update the target sequence (length 1)\n",
        "            target_seq = np.zeros((1, 1))\n",
        "            target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Clean up the output\n",
        "        result = decoded_sentence.strip()\n",
        "\n",
        "        # Remove repetitive patterns\n",
        "        words = result.split()\n",
        "        cleaned_words = []\n",
        "        for i, word in enumerate(words):\n",
        "            if i == 0 or word != words[i-1]:\n",
        "                cleaned_words.append(word)\n",
        "\n",
        "        result = ' '.join(cleaned_words)\n",
        "\n",
        "        # If result is empty or just repetitive words, return a message\n",
        "        if not result or len(set(result.split())) <= 1:\n",
        "            # Try to find partial matches in common translations\n",
        "            for key in common_translations:\n",
        "                if key in input_text or input_text in key:\n",
        "                    return common_translations[key] + \" (partial match)\"\n",
        "\n",
        "            # If no partial match, use the first word of input to find a match\n",
        "            input_words = input_text.split()\n",
        "            if input_words:\n",
        "                first_word = input_words[0]\n",
        "                for key in common_translations:\n",
        "                    if key.startswith(first_word) or first_word.startswith(key):\n",
        "                        return common_translations[key] + \" (word match)\"\n",
        "\n",
        "            return \"Could not generate a meaningful translation.\"\n",
        "\n",
        "        print(f\"Translation completed in {time.time() - start_time:.2f} seconds\")\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Translation error: {e}\")\n",
        "        # Fall back to common translations\n",
        "        for key in common_translations:\n",
        "            if key in input_text or input_text in key:\n",
        "                return common_translations[key] + \" (error fallback)\"\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "# Step 10.3: Create a simple text-based translation interface (no widgets)\n",
        "print(\"\\n--- Kikuyu to Kiswahili Translator ---\")\n",
        "print(\"Enter 'q' to quit the translator\")\n",
        "\n",
        "# Define some simple test words that should work well\n",
        "test_sentences = [\n",
        "    \"Ngai\",  # God\n",
        "    \"mũndũ\",  # person\n",
        "    \"maaĩ\",  # water\n",
        "    \"thĩ\",  # earth\n",
        "    \"mũthenya\"  # day\n",
        "]\n",
        "\n",
        "print(\"\\nTesting translation with simple words:\")\n",
        "for sentence in test_sentences:\n",
        "    translation = translate_text(sentence)\n",
        "    print(f\"Kikuyu: {sentence}\")\n",
        "    print(f\"Kiswahili: {translation}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# Interactive translation loop\n",
        "print(\"\\n=== Interactive Translation Interface ===\")\n",
        "print(\"Type short Kikuyu words or phrases (1-5 words work best)\")\n",
        "while True:\n",
        "    user_input = input(\"\\nEnter Kikuyu text to translate (or 'q' to quit): \")\n",
        "\n",
        "    if user_input.lower() == 'q':\n",
        "        print(\"Exiting translator. Thank you!\")\n",
        "        break\n",
        "\n",
        "    if not user_input.strip():\n",
        "        print(\"Please enter some text to translate.\")\n",
        "        continue\n",
        "\n",
        "    print(\"Translating...\")\n",
        "    translation = translate_text(user_input)\n",
        "    print(f\"\\nKikuyu: {user_input}\")\n",
        "    print(f\"Kiswahili: {translation}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "print(\"\\nTranslation session completed.\")"
      ],
      "metadata": {
        "id": "B3xIw0dHnbJ-",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add this to your Google Colab notebook\n",
        "!pip install flask flask-cors pyngrok\n",
        "\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "import threading\n",
        "from google.colab import output\n",
        "import nest_asyncio\n",
        "\n",
        "# Apply nest_asyncio to allow nested event loops\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Create Flask app\n",
        "app = Flask(__name__)\n",
        "CORS(app)  # Enable CORS for all routes\n",
        "\n",
        "# Define translation endpoint that will use your existing translation function\n",
        "@app.route('/translate', methods=['POST'])\n",
        "def translate_api():\n",
        "    data = request.json\n",
        "    text = data.get('text', '')\n",
        "\n",
        "    # Use your existing translation function\n",
        "    translation = translate_text(text)\n",
        "\n",
        "    return jsonify({'translation': translation})\n",
        "\n",
        "# Function to run the Flask app\n",
        "def run_flask():\n",
        "    app.run(host='0.0.0.0', port=8000)\n",
        "\n",
        "# Start the Flask app in a separate thread\n",
        "threading.Thread(target=run_flask).start()\n",
        "\n",
        "# Use ngrok to expose the local server to the internet\n",
        "!pip install pyngrok\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Set up ngrok with authentication\n",
        "# You need to sign up at https://dashboard.ngrok.com/signup\n",
        "# Then get your authtoken from https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "ngrok_auth_token = \"2woKs9KZbRrC2icpYM9eUJxbZhP_LFmZgX7nwey1z9hEU7E8\"  # Replace with your actual token\n",
        "ngrok.set_auth_token(ngrok_auth_token)\n",
        "\n",
        "# Now connect to ngrok\n",
        "public_url = ngrok.connect(8000)\n",
        "print(f\"Public URL: {public_url}\")"
      ],
      "metadata": {
        "id": "a_irK-kG_j1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#REMEDIES OF FIXING THE TRANSLATION ISSUES .\n",
        "\n",
        "#*In google colab.\n",
        "#1.In the Google colab Runtime section, change the runtime type from CPU to GPU then after sometime change it back to CPU.\n",
        "\n",
        "#2.Then still in the Runtime section, choose the Restart session option.\n",
        "\n",
        "#*In NGRock\n",
        "#(After signing in and going to the official dashboard)\n",
        "#1. On the left side, scroll down and locate settings,click settings and scroll down and locate \"Revoke sessions\" button.\n",
        "\n",
        "#2. Go back to the the main dashboard where your Auth token is and scroll down till bottom and locate \"Reset Authtoken\" button.\n",
        "\n",
        "#3. Copy the new Auth token and go back to google colab and paste it where it is supposed to be placed .\n",
        "\n",
        "#4. Finally after you have done all the above steps, Click Runtime and Click \"Run All\" in order to run all the google colab processes again and the issue will be fixed.\n",
        "\n",
        "#* At the end of the process you should be provided with a public URL which you are supposed to copy it in the your VS code , Javascript section.\n",
        "\n",
        "#* Run the live server and then use the words in the local dictionary for translation."
      ],
      "metadata": {
        "id": "3En31hZR-UFl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNm+cQ6uFW1I/dBORtTCF9I",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}